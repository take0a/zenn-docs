---
title: "SageMaker Studio Lab を使ってみる"
emoji: "🧠"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["aws", "機械学習", "ai", "sagemaker"]
published: false
publication_name: "robon"
---

# はじめに
AIの本で、notebookを公開しているものは、だいたい Google Colaboratory 前提なんだけど、今回は、AWS 上の SaaS 製品を AI 武装しようかな（できるかな？）なので、SageMaker 上で動かそうということで、調べていると、SageMaker にも SageMaker Studio Lab というものがあるようで、ま、やってみようということです。

# やってみる
## 申請
https://studiolab.sagemaker.aws/

Request free account します。メールがくるので verify します。あらら、すぐには使えないのね。`Account request approved`メールが届くと使えます。私は、翌日に届きました。

このメールで`Create account`ボタンを押すと、ユーザー名やパスワードが指定できます。もう一回 verify があってから、サインインできるようになります。

## サインイン
サインインして、`Conpute type`を`CPU`か`GPU`から選んで、`Start runtime`します。CPU も GPU も4時間使えるようです。24時間あたりだと CPU は8時間、GPU は4時間が上限のようです。

しばらくすると、`Open project`できるようになりますので、`Open project`します。

どうも、人間かどうかチェックのクイズのパネルが出せなくてのたうち回っているようです。生暖かく見守りましょう。

## LLM本
`Open project`すると、JupyterLab が開くので、ちょうどやろうと思っていた LLM本のリポジトリを動かしてみます。
https://github.com/ghmagazine/llm-book

こちらは、Google Colab 用に書かれているので、これを fork して、変更が必要な場合は、反映していきます。
https://github.com/take0a/llm-book

同じようにやりたくて、かつ、面倒なことを避けたい場合は、オリジナルではなく、私のリポジトリを fork してもよいでしょう。

そして、このリポジトリを JupyterLab へ clone します。clone する先は、空欄にすると root ディレクトリになります。私のリポジトリを fork した人は、`Search for environment.yml and build Conda environment` のチェックをしたまま行うと `conda env create` してくれますので、書かれているとおり、`conda activate llm-book`　しておきます。

LLM本は、Python 3.10 を期待しているようなので、default が 3.9 の場合は、新しい環境に切り替えておくとよいでしょう。でないと、Chapter 5で詰みます。

## 日本語化する
https://github.com/aws-sagemaker-jp/awesome-studio-lab-jp/blob/main/README_usage.md#5-studio-lab%E3%82%92%E3%82%88%E3%82%8A%E4%BE%BF%E5%88%A9%E3%81%AB%E4%BD%BF%E3%81%86

~~書かれているとおりにすれば、日本語化できます。~~
できませんでした。。。

### Chapter1
`chapter1/1-introduction.ipynb`を開きます。`Select Kernel`では、`llm-book:Python`を選びます。
どんどん実行できます。が、最後まで実行して、github へ push する際に、ユーザー名とパスワードを指定しても error になります。

remote: Support for password authentication was removed on August 13, 2021. remote: Please see https://docs.github.com/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication. fatal: Authentication failed for 'https://github.com/take0a/llm-book.git/'

以下を見て access token を作成して、ユーザー名とトークンを指定すると、push できます。

https://docs.github.com/ja/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens

以降も、どんどん実行できます。

### Chapter5
5.2.8 からは GPU でないと動かない設定になっているので、GPU にしましょう。
書籍では、`Colab の GPU 環境でおよそ3時間かかります。`となっていますが、5分ぐらいで終わりました。

5.2.10 の Google Drive と Hugging Face Hub は省略します。（以降同じ）

また、MARC-ja は、2024/4/16 時点でもエラーだったので、エラーのままにしました。

5.4.1 は、OutOfMemoryError になりました。（どのプロセスがどんだけ使ってるとか言ってるので、runtime を殺して、再スタートしたら動きました）

OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.58 GiB of which 19.62 MiB is free. Process 4862 has 4.41 GiB memory in use. Process 5342 has 608.00 MiB memory in use. Process 5513 has 578.00 MiB memory in use. Process 5588 has 816.00 MiB memory in use. Process 5857 has 8.00 GiB memory in use. Process 6060 has 196.00 MiB memory in use. Of the allocated memory 96.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

### Chapter6
モデルのファインチューニングで OutOfMemoryError になりました。（File System Full もくるので、きれいにしながら進めます。runtimeを殺さなくても、カレント以外の Kernelを殺せばいけるかも）

OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.58 GiB of which 13.62 MiB is free. Process 4816 has 608.00 MiB memory in use. Process 4996 has 3.67 GiB memory in use. Process 5184 has 608.00 MiB memory in use. Process 5281 has 3.66 GiB memory in use. Process 5417 has 3.62 GiB memory in use. Process 5884 has 2.43 GiB memory in use. Of the allocated memory 2.20 GiB is allocated by PyTorch, and 99.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

#### 失敗例
以下、最初に実行したときの話です。

ここまでは、`Select Kernel`で`default:Python`で実行してきましたが、Chapter5 では、Python 3.10 以降の文法がつかわれていましたので、conda 環境をつくることにしました。

https://github.com/take0a/llm-book/blob/main/environment.yml

を追加して、このファイルを選んで、conda 環境を作りましたが、複数の環境にライブラリをインストールしまくると、容量がパンクするので、既存の環境を削除しないと動かないかもしれません。

Python 3.10 問題は回避できても、5.2.8. には、GPU問題もありますので、GPU に切り替えます。

それでも、ディスクがパンクするので、キャッシュをクリアします。

```
$ pip cache purge
$ rm -rf ~/.cache/huggingface/*
```

### Chapter7
ここは、File System Full を回避できませんでしたので、遺体となったモデルを消して、続行してあります。

### Chapter8
GPU の残り時間を気にしながら実行しましょう。

### Chapter9
OpenAI の API は省略しました。

## リセット
https://github.com/aws/studio-lab-examples/issues/75#issuecomment-1326660232

書かれているとおり conda 環境を削除して、リセットします。
File System Full となっている場合は、ユーザーファイルを消し、また、キャッシュファイルをクリアするとキレイになります。

```bash
$ pip cache purge
$ rm -rf ~/.cache/huggingface/*
```

# おわりに
Python のバージョンにはじまり、CPU と GPU、OutOfMemoryError に File System Full とにぎやかなので、JupyterLab もいいけど、タスクごとにコンテナを割り当てて、本物の SageMaker を使うのがよろしいようです。

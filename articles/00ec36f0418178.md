---
title: "AOSS ã¨ Bedrock ã§ AI æ¤œç´¢"
emoji: "ğŸ§ "
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["opensearch", "aoss", "bedrock", "embedding", "rag"]
published: false
publication_name: "robon"
---
# ã¯ã˜ã‚ã«
AOSS ã¨ã¯ã€Amazon OpenSearch Serverless ã§ã‚ã‚‹ã€‚å¼Šç¤¾ã§ã¯ã€è£½å“å†…ã®å…¨æ–‡æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã¨ã—ã¦ã€AOSS ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã€‚ã‚„ã¯ã‚Šã€é‹ç”¨çš„ã«ã‚‚ä¿å®ˆçš„ã«ã‚‚ã€è¦æ±‚é‡ã®å¢—æ¸›ã‚„AZéšœå®³ã‚’æ°—ã«ã—ãªãã¦è‰¯ã„ã€ŒçœŸã«ãƒ•ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ‰ã€ã®ä¸–ç•Œã‚’çµŒé¨“ã—ã¦ã—ã¾ã†ã¨ã€ã‚‚ã†å…ƒã«ã¯æˆ»ã‚Œãªã„ã€‚

ä¸–ã®ä¸­ã¯ã€ã‚„ã¯ã‚Š AI ã§ã‚ã‚‹ã€‚ãªã‚“ã¨ã‹ã—ã‚ˆã†ã¨ [Bedrock Knowledgebase](https://aws.amazon.com/jp/bedrock/knowledge-bases/) ãªã©ã‚’èª¿æŸ»ã—ã¦ã¿ãŸã®ã§ã‚ã‚‹ãŒã€åˆ¥ã« Chat ãªã‚“ã‹ã—ãªãã¦ã‚‚ã€AI æ¤œç´¢ãŒã§ãã‚Œã°ã„ã„ã‚“ã ã‚ˆãªãâ€¦ã¨æ€ã£ã¦ã„ãŸã¨ã“ã‚ã€

https://aws.amazon.com/jp/about-aws/whats-new/2025/08/amazon-opensearch-serverless-ai-connectors-hybrid-search/

ã§ã‚ã‚‹ã€‚2025å¹´8æœˆ7æ—¥ã€‚ã“ã‚Œã‹ã‚‚ã—ã‚Œãªã„â€¦

# ã‚„ã£ã¦ã¿ãŸ
ã¨ã¯ã„ãˆã€ä½•ã®çŸ¥è­˜ã‚‚ãªã„çŠ¶æ…‹ã§å§‹ã‚ã‚‰ã‚Œã‚‹ã»ã©ã€ç”˜ãã¯ãªã„ã“ã¨ã¯æ‰¿çŸ¥ã—ã¦ã‚‹ã®ã§ã€ã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ã‚’æ¢ã™ã“ã¨ã«ã—ãŸã€‚

## ã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹
ã‚ã£ãŸã€‚

https://aws.amazon.com/jp/blogs/news/introduction-to-amazon-opensearch-service-workshop-jp

2025å¹´3æœˆ21æ—¥ã€‚ã„ã„ã‹ã‚‚ã—ã‚Œãªã„â€¦

## çœŸã«ãƒ•ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ‰
OpenSearch Service ã‚’ OpenSearch Serverless ã«ç½®ãæ›ãˆã€SageMager ã‚’ Bedrock ã«ç½®ãæ›ãˆã‚ˆã†ã€‚

## çµè«–
ç½®ãæ›ãˆãŸã€‚

https://github.com/take0a/aoss-bedrock-workshop

ä»¥ä¸Šã€‚ã§ã‚ã‚‹ãŒã€ã“ã“ã§ã‚‚èª¬æ˜ã—ã¦ã¿ã‚‹ã€‚ã“ã“ã§ã¯ã€ `4-ai-search.ipynb` ã ã‘ã«ã—ã¾ã™ãŒã€ä»–ã®å†…å®¹ã‚‚å¤§å¤‰å‹‰å¼·ã«ãªã£ãŸã€‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚

## æº–å‚™
ç’°å¢ƒã¯ã€ã“ã‚“ãªæ„Ÿã˜ã§ã€VSCode ã‹ã‚‰ EC2 ã« Remote Development ã§ Jupyter Notebook ã‚’é–‹ã„ã¦ã€EC2 ã® IAM ãƒ­ãƒ¼ãƒ«ã§ AOSS ã¨ Bedrock ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¾ã™ã€‚ï¼ˆãªã®ã§ã€CloudFormation ã¨ã‹ CDK ã¯ä½¿ã„ã¾ã›ã‚“ï¼‰

![](/images/00ec36f0418178/architecture-with-bedrock.png =500x)

### EC2
æœ€è¿‘ã¯ Python ã¨ã„ãˆã°ã€`uv` ãªã®ã§ã€[ã“ã“](https://docs.astral.sh/uv/getting-started/installation/)ã¿ã¦ã€uv ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚

ä¸Šã®ãƒªãƒã‚¸ãƒˆãƒªã‚’ä½¿ã†ãªã‚‰ã€
```
git clone https://github.com/take0a/aoss-bedrock-workshop.git
cd aoss-bedrock-workshop
uv sync
```

VSCode ã«ã¯ã€ä»¥ä¸‹ã® Extension ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚
- [Python](https://marketplace.visualstudio.com/items?itemName=ms-python.python)
- [Jupyter](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter)

### IAM
ä»¥ä¸‹ã® IAM Role ã« AOSS ã¨ Bedrock ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚’ä»˜ä¸ã—ã¦ãã ã•ã„ã€‚
- ä¸Šè¨˜ã®è¸ã¿å°ã«ã™ã‚‹ EC2 ã«ç´ã¥ã‘ãŸ IAM Role
- ç®¡ç†ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‚’æ“ä½œã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼

ã“ã‚Œã¨ã¯åˆ¥ã«ã€AOSS ãŒ Bedrock ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã® IAM Role ã‚’ä½œæˆã—ã¾ã™ã€‚ã“ã“ã§ã€Œä¿¡é ¼ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã€ã¯ã€`ml.opensearchservice.amazonaws.com` ãªã®ã§æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚ã¨ã„ã†ã‹çµæ§‹ç„¡ç†ã‚²ãƒ¼ã§ã™ã€‚[ã“ã“](https://docs.aws.amazon.com/ja_jp/opensearch-service/latest/developerguide/ml-amazon-connector.html)ã«æ›¸ã„ã¦ã‚ã‚Šã¾ã™ãŒâ€¦ã€‚ãƒãƒªã‚·ãƒ¼ã¯ã€InvokeModel ãŒã§ãã‚Œã°è‰¯ã„ã§ã™ã€‚

### AOSS
ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œã‚‹ã®ã§ã™ãŒã€ä½œã‚‹å‰ã«ã€AOSS ã®ãƒãƒªã‚·ãƒ¼ã‚’ä½œã‚Šã¾ã™ã€‚ãã®å‰ã«ã€ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®åå‰ã‚’æ±ºã‚ã¦ãŠãã¾ã™ã€‚ã“ã“ã§ã¯ã€`workshop-vector` ã§äºˆå®šã—ã¾ã™ã€‚ã“ã‚Œã¯ã€ãƒãƒªã‚·ãƒ¼ã‚’å‰²ã‚Šå½“ã¦ã‚‹ãƒªã‚½ãƒ¼ã‚¹ã‚’ç‰¹å®šã™ã‚‹ãŸã‚ã§ã™ã€‚

#### Encryption policy
ãƒªã‚½ãƒ¼ã‚¹ã‚’ `workshop*` ã®ã‚ˆã†ã«ã€ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åã‚’ã‚«ãƒãƒ¼ã™ã‚‹ã‚ˆã†ã«è¨­å®šã—ã¾ã™ã€‚æš—å·åŒ–ã¯ã€ä»–ã®ã‚­ãƒ¼ã§ã‚‚è‰¯ã„ã§ã™ãŒã€AWSæ‰€æœ‰ã‚­ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã§è‰¯ã„ã§ã™ã€‚

#### Network policy
`workshop*` ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨ã‚¢ã‚¯ã‚»ã‚¹ã‚’æœ‰åŠ¹ã«ã—ã¾ã™ã€‚ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆã™ã‚‹ã ã‘ãªã‚‰ã€ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã§ã‚‚è‰¯ã„ã§ã—ã‚‡ã†ã€‚

#### Data access policy
ãƒ—ãƒªãƒ³ã‚·ãƒ‘ãƒ«ã¯ã€ä¸Šã® IAM ã§ AOSS ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚’ä»˜ä¸ã—ãŸ EC2 ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã® Role ã«ã—ã¾ã™ã€‚ãƒªã‚½ãƒ¼ã‚¹ã¯ã€`collection/workshop*` ã¨ `index/workshop*/*` ã¨ `model/workshop*/*` ã®ï¼“ã¤ã§ã™ãŒã€å°ã•ã„ç”»é¢ã§ã¯ã€æœ€å¾Œã® model ãŒè¦‹ãˆãªã„ï¼†ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ãƒãƒ¼ã‚‚å‡ºãªã„çŠ¶æ…‹ã«ãªã‚Šã¾ã™ã€‚ã¾ãŸã€ï¼“ã¤è¨±å¯ã—ãŸã„ã®ã«ã€ï¼‘ãƒ«ãƒ¼ãƒ«ã®ã‚¢ã‚¤ãƒ†ãƒ ã¯ï¼’ã¤ã¾ã§ã¨æ€’ã‚‰ã‚Œã¾ã™ã€‚ã“ã“ã¯ã€ã‚°ãƒƒã¨å ªãˆã¦ã€ãƒ«ãƒ¼ãƒ«ã‚’ï¼’ã¤ã«ã—ã¾ã™ã€‚

### Bedrock
`Amazon Titan Text Embeddings V2` ã‚’ä½¿ã„ãŸã„ã®ã§ã€ç®¡ç†ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã® `ãƒ¢ãƒ‡ãƒ«ã‚¢ã‚¯ã‚»ã‚¹` ã§ `ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¯èƒ½` ã®å ´åˆã«ã¯ã€`ãƒ¢ãƒ‡ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆ` ã—ã¦ã€`ã‚¢ã‚¯ã‚»ã‚¹ãŒä»˜ä¸ã•ã‚Œã¾ã—ãŸ` ã«ã—ã¾ã™ã€‚

## ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¯ã€Bedrock Knowledgebase ã§ã‚‚åˆ©ç”¨å¯èƒ½ã§ã—ãŸã®ã§ã€2025å¹´8æœˆ7æ—¥ã®ç™ºè¡¨åˆ†ã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€åŸºæœ¬ãªã®ã§ã€ã“ã“ã‹ã‚‰ã‚„ã‚Šã¾ã™ã€‚

### Bedrock å‘¼ã³å‡ºã—ã®ãƒã‚§ãƒƒã‚¯
[ã“ã“](https://docs.aws.amazon.com/ja_jp/bedrock/latest/userguide/bedrock-runtime_example_bedrock-runtime_InvokeModelWithResponseStream_TitanTextEmbeddings_section.html) ã®ã‚³ãƒ”ãƒšã§ã™ãŒã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ã£ã¦ã€Bedrock ã®è¨­å®šã‚„ IAM ã®è¨­å®šã‚’ç¢ºèªã—ã¾ã™ã€‚

```py
# Generate and print an embedding with Amazon Titan Text Embeddings V2.

import boto3
import json

# Create a Bedrock Runtime client in the AWS Region of your choice.
client = boto3.client("bedrock-runtime", region_name="us-east-1")

# Set the model ID, e.g., Titan Text Embeddings V2.
model_id = "amazon.titan-embed-text-v2:0"

# The text to convert to an embedding.
input_text = "Please recommend books with a theme similar to the movie 'Inception'."

# Create the request for the model.
native_request = {"inputText": input_text}

# Convert the native request to JSON.
request = json.dumps(native_request)

# Invoke the model with the request.
response = client.invoke_model(modelId=model_id, body=request)

# Decode the model's native response body.
model_response = json.loads(response["body"].read())

# Extract and print the generated embedding and the input text token count.
embedding = model_response["embedding"]
input_token_count = model_response["inputTextTokenCount"]

print("\nYour input:")
print(input_text)
print(f"Number of input tokens: {input_token_count}")
print(f"Size of the generated embedding: {len(embedding)}")
print("Embedding:")
print(embedding)
```

### AOSS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
ä»¥ä¸‹ã®ã‚ˆã†ã«ã€`knn_vector` ã‚¿ã‚¤ãƒ—ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ç”¨æ„ã—ãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚

```py
payload = {
  "mappings": {
    "properties": {
      "id": {"type": "keyword"},
      "question": {"type": "text", "analyzer": "custom_kuromoji_analyzer"},
      "context":  {"type": "text", "analyzer": "custom_kuromoji_analyzer"},
      "answers":  {"type": "text", "analyzer": "custom_kuromoji_analyzer"},
      "question_embedding": {
        "type": "knn_vector",
        "dimension": 1024,
        "space_type": "l2",
        "method": {
          "name": "hnsw",
          "engine": "faiss",
        }
      },
      "context_embedding": {
        "type": "knn_vector",
        "dimension": 1024,
        "space_type": "l2",
        "method": {
          "name": "hnsw",
          "engine": "faiss",
        },
      }
    }
  },
  "settings": {
    "index.knn": True,
    "analysis": {
      "analyzer": {
        "custom_kuromoji_analyzer": {
          "char_filter": ["icu_normalizer"],
          "filter": [
              "kuromoji_baseform",
              "custom_kuromoji_part_of_speech"
          ],
          "tokenizer": "kuromoji_tokenizer",
          "type": "custom"
        }
      },
      "filter": {
        "custom_kuromoji_part_of_speech": {
          "type": "kuromoji_part_of_speech",
          "stoptags": ["æ„Ÿå‹•è©,ãƒ•ã‚£ãƒ©ãƒ¼","æ¥é ­è¾","ä»£åè©","å‰¯è©","åŠ©è©","åŠ©å‹•è©","å‹•è©,ä¸€èˆ¬,*,*,*,çµ‚æ­¢å½¢-ä¸€èˆ¬","åè©,æ™®é€šåè©,å‰¯è©å¯èƒ½"]
        }
      }
    }
  }
}
# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åã‚’æŒ‡å®š
index_name = "jsquad-knn"

try:
    # æ—¢ã«åŒåã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ã„ã£ãŸã‚“å‰Šé™¤ã‚’è¡Œã†
    print("# delete index")
    response = opensearch_client.indices.delete(index=index_name)
    print(json.dumps(response, indent=2))
except Exception as e:
    print(e)

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
response = opensearch_client.indices.create(index_name, body=payload)
response
```

### ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®å ´åˆã€ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã¯ã€AOSS å†…éƒ¨ã‹ã‚‰ã§ã¯ãªãã€ä»¥ä¸‹ã®ã‚ˆã†ã«ã€AOSS ã¸ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹å‰ã«è¡Œã„ã¾ã™ã€‚

```py
def get_df_with_embeddings(input_df, field_mappings, model_id, bedrock_region, batch_size):
    output_df = pd.DataFrame([]) #create empty dataframe
    df_list = np.array_split(input_df, input_df.shape[0]/batch_size)
    for df in tqdm(df_list):
        index = df.index #backup index number
        df_with_embeddings = df
        for field_mapping in field_mappings:
            input_field_name = field_mapping["InputFieldName"]
            embedding_field_name = field_mapping["EmbeddingFieldName"]
            payload = {
                "inputText": df_with_embeddings[input_field_name].values.tolist()[0]
            }
            body = json.dumps(payload)
            bedrock_runtime_client = boto3.client("bedrock-runtime", region_name=bedrock_region)
            response = bedrock_runtime_client.invoke_model(modelId=model_id, body=body)
            model_response = json.loads(response["body"].read())
            embeddings = [model_response["embedding"]]

            df_with_embeddings = pd.concat([df_with_embeddings.reset_index(drop=True), pd.Series(embeddings,name=embedding_field_name).reset_index(drop=True)],axis=1) #join embedding results to source dataframe
            df_with_embeddings = df_with_embeddings.set_index(index) #restore index number

        output_df = pd.concat([output_df, df_with_embeddings])
    return output_df

valid_df_with_embeddings = get_df_with_embeddings(
    input_df=valid_df,
    field_mappings=[
        {"InputFieldName": "question", "EmbeddingFieldName": "question_embedding"},
        {"InputFieldName": "context", "EmbeddingFieldName": "context_embedding"},
    ],
    model_id=model_id,
    bedrock_region=default_region,
    batch_size=1
)
```

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ­ãƒ¼ãƒ‰
ä¸Šã§ã€Pandas ã® DataFrame ä¸Šã«æº–å‚™ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```py
index_name = "jsquad-knn"
response = wr.opensearch.index_df(
    client=opensearch_client,
    df=valid_df_with_embeddings,
    use_threads=True,
    index=index_name,
    bulk_size=200, # 200 ä»¶ãšã¤æ›¸ãè¾¼ã¿
    refresh=False,
)
```

### ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ™ã‚¯ãƒˆãƒ«ã‚‚å¤–éƒ¨ã§ä½œæˆã—ã¦ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸãŒã€æ¤œç´¢ã®å…¥åŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚å¤–éƒ¨ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦æ¤œç´¢ã—ã¾ã™ã€‚

```py
index_name = "jsquad-knn"
model_id = "amazon.titan-embed-text-v2:0"
query = "æ—¥æœ¬ã§æ¢…é›¨ãŒãªã„å ´æ‰€ã¯ï¼Ÿ"

def text_to_embedding(text, region_name, model_id):
    payload = {
        "inputText": text
    }
    body = json.dumps(payload)
    bedrock_runtime_client = boto3.client("bedrock-runtime", region_name)
    response = bedrock_runtime_client.invoke_model(modelId = model_id, body=body)
    model_response = json.loads(response["body"].read())
    return model_response["embedding"]

vector = text_to_embedding(text=query, region_name=default_region, model_id=model_id)
k = 10

payload = {
  "query": {
    "knn": {
      "question_embedding": {
        "vector": vector,
        "k": k
      }
    }
  },
  "_source": False,
  "fields": ["question", "answers", "context"],
  "size": k
}
response = opensearch_client.search(
    index=index_name,
    body=payload
)
pd.json_normalize(response["hits"]["hits"])
```

## ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ¤œç´¢
ã‚„ã£ã¨ã€2025å¹´8æœˆ7æ—¥ã®ç™ºè¡¨åˆ†ã§ã™ã€‚
ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨ä½•ãŒé•ã†ã®ã‹ï¼Ÿãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²æ™‚ã®ãƒ™ã‚¯ãƒˆãƒ«ã®ä½œæˆã‚‚ã€æ¤œç´¢æ™‚ã®ãƒ™ã‚¯ãƒˆãƒ«ã®ä½œæˆã‚‚ AOSS å´ã§ã‚„ã£ã¦ãã‚Œã‚‹ã®ãŒã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ¤œç´¢ã§ã™ã€‚

### ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨åŒã˜ã§ã™ã€‚Workshop ã§ã¯ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®åå‰ã ã‘ `jsquad-neural-search` ã«å¤‰æ›´ã—ã¦ã„ã¾ã™ã€‚

### ã‚³ãƒã‚¯ã‚¿ã®ä½œæˆ
ã“ã‚Œã‚‚ 2025å¹´8æœˆ7æ—¥ã®ç™ºè¡¨åˆ†ã§ã™ã€‚AOSS ãŒ Bedrock ã¸ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã®ã‚³ãƒã‚¯ã‚¿ã‚’ä½œæˆã—ã¾ã™ã€‚

```py
embedding_model_name = "amazon.titan-embed-text-v2:0"

payload = {
  "name": embedding_model_name, 
  "description": "Remote connector for " + embedding_model_name,
  "version": 1, 
  "protocol": "aws_sigv4",
  "credential": {
    "roleArn": opensearch_connector_role_arn
  },
  "parameters": {
    "region": default_region,
    "service_name": "bedrock",
    "model": embedding_model_name,
    "dimensions": 1024,
    "normalize": True,
    "embeddingTypes": ["float"],    
  },
  "actions": [
    {
      "action_type": "predict",
      "method": "POST",
      "headers": {
          "content-type": "application/json",
          "x-amz-content-sha256": "required",
      },
      "url": "https://bedrock-runtime.${parameters.region}.amazonaws.com/model/${parameters.model}/invoke",
      "pre_process_function": "connector.pre_process.bedrock.embedding",
      "request_body": '{ "inputText": "${parameters.inputText}", "dimensions": ${parameters.dimensions}, "normalize": ${parameters.normalize}, "embeddingTypes": ${parameters.embeddingTypes} }',
      "post_process_function": "connector.post_process.bedrock.embedding",
    }
  ]
}

# API ã®å®Ÿè¡Œ
response = opensearch_client.http.post("/_plugins/_ml/connectors/_create", body=payload)

# çµæœã‹ã‚‰ã‚³ãƒã‚¯ã‚¿ ID ã‚’å–å¾—
opensearch_embedding_connector_id = response["connector_id"]
print("embedding connector id: " + opensearch_embedding_connector_id)
```

### ãƒ¢ãƒ‡ãƒ«ã®ç™»éŒ²
ä¸Šã®ã‚³ãƒã‚¯ã‚¿ã‚’æŒ‡å®šã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²ã—ã¾ã™ã€‚ã“ã“ã‹ã‚‰å…ˆã¯ã€ãƒ¢ãƒ‡ãƒ«ã®IDã§å‚ç…§ã•ã‚Œã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚

```py
payload = {
    "name": embedding_model_name,
    "description": embedding_model_name,
    "function_name": "remote",
    "connector_id": opensearch_embedding_connector_id
}
response = opensearch_client.http.post("/_plugins/_ml/models/_register?deploy=true", body=payload)

opensearch_embedding_model_id = response['model_id']

for i in range(300):
    ml_model_status = opensearch_client.http.get("/_plugins/_ml/models/"+ opensearch_embedding_model_id)
    model_state = ml_model_status.get("model_state")
    if model_state in ["DEPLOYED", "PARTIALLY_DEPLOYED"]:
        break
    time.sleep(1)

if model_state == "DEPLOYED":
    print("embedding model " + opensearch_embedding_model_id + " is deployed successfully")
elif model_state == "PARTIALLY_DEPLOYED":
    print("embedding model " + opensearch_embedding_model_id + " is deployed only partially")
else:
    raise Exception("embedding model " + opensearch_embedding_model_id + " deployment failed")

print(ml_model_status)
```

ãã‚‹ãã‚‹ã—ã¦ã‚‹ã®ã§ã€é•·ã„ã§ã™ãŒã€æœ€åˆã®ï¼—è¡Œã§ã™ã€‚

### Ingestion pipeline


# ãŠã‚ã‚Šã«
